#!/usr/bin/env python3
"""
Streamlit app for PDF fixture table extraction using Gemini
"""

import streamlit as st
import pandas as pd
import pdfplumber
import google.generativeai as genai
import json
import re
import os
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from io import BytesIO
import concurrent.futures
import time
import hashlib

# Load environment variables
load_dotenv()

# Page config
st.set_page_config(
    page_title="å»ºå…·è¡¨æŠ½å‡ºã‚¢ãƒ—ãƒª",
    page_icon="ğŸ—ï¸",
    layout="wide"
)

class LLMFixtureParser:
    def __init__(self, api_key=None):
        # Geminiè¨­å®š
        api_key = api_key or os.getenv('GEMINI_API_KEY')
        if api_key:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-2.5-flash')
        else:
            raise ValueError("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.system_prompt = """ã‚ãªãŸã¯å»ºç¯‰å›³é¢ã®å»ºå…·è¡¨ã‚’è§£æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å»ºå…·æƒ…å ±ã‚’æŠ½å‡ºã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸ**JSONå½¢å¼**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯å¿…ãšJSONå½¢å¼ã§ã€jsonã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

        self.extract_prompt_template = """
ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
    "fixtures": [
        {{
            "è¨˜å·": "SS-3",
            "æ•°é‡": 2,
            "å®¤å": "ï¼‘ï¼¦ï¼šç ”ä¿®å®Ÿç¿’å®¤",
            "å‹å¼": "é›»å‹•ã‚·ãƒ£ãƒƒã‚¿ãƒ¼",
            "è¦‹è¾¼": "",
            "å§¿å›³": "",
            "ä»•ä¸Šã’": "",
            "ã‚¬ãƒ©ã‚¹": "",
            "ä»˜å±é‡‘ç‰©": "æ¨™æº–é‡‘ç‰©ä¸€å¼",
            "å‚™è€ƒ": "æ–‡åŒ–ã‚·ãƒ£ãƒƒã‚¿ãƒ¼ï¼šå¾¡å‰æ§˜"
        }},
        ...
    ]
}}
"""

    def extract_with_gemini(self, text: str) -> List[Dict[str, Any]]:
        """Gemini APIã‚’ä½¿ç”¨ã—ã¦æŠ½å‡º"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆ
            prompt = self.system_prompt + "\n\n" + self.extract_prompt_template.format(text=text)

            response = self.model.generate_content(prompt)
            result_text = response.text

            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                result = json.loads(json_match.group())
                return result.get('fixtures', [])

        except Exception:
            # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è»½é‡åŒ–ï¼ˆUIæ›´æ–°ãªã—ï¼‰
            pass

        return []

    def process_pdf_page(self, page: Any, page_num: int = 0) -> List[Dict[str, Any]]:
        """å˜ä¸€ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†"""
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        text = page.extract_text()
        if not text:
            return []

        # å»ºå…·è¡¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è­˜åˆ¥
        if 'å»ºå…·è¡¨' in text or 'è¨˜å·' in text:
            # Geminiã§æŠ½å‡º
            fixtures = self.extract_with_gemini(text)
            # page_numã‚’ãƒ­ã‚°ç”¨ã«ä½¿ç”¨ï¼ˆæœªä½¿ç”¨è­¦å‘Šã‚’å›é¿ï¼‰
            _ = page_num
            return fixtures

        return []

    def process_page_parallel(self, page_info: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒšãƒ¼ã‚¸å‡¦ç†é–¢æ•°"""
        page_num = page_info['page_num']
        text = page_info['text']

        start_time = time.time()
        print(f"ğŸ”¥ ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹: ãƒšãƒ¼ã‚¸ {page_num} ã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œé–‹å§‹ ({start_time:.3f})")

        try:
            print(f"ğŸŒ APIå‘¼ã³å‡ºã—é–‹å§‹: ãƒšãƒ¼ã‚¸ {page_num} Gemini APIå‘¼ã³å‡ºã—é–‹å§‹ ({time.time():.3f})")
            fixtures = self.extract_with_gemini(text)
            end_time = time.time()
            duration = end_time - start_time
            print(f"âœ… APIå®Œäº†: ãƒšãƒ¼ã‚¸ {page_num} å‡¦ç†å®Œäº† ({end_time:.3f}) å‡¦ç†æ™‚é–“: {duration:.3f}ç§’")
            return {
                'page_num': page_num,
                'fixtures': fixtures,
                'success': True,
                'error': None,
                'start_time': start_time,
                'end_time': end_time,
                'duration': duration
            }
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ ({end_time:.3f}) å‡¦ç†æ™‚é–“: {duration:.3f}ç§’")
            return {
                'page_num': page_num,
                'fixtures': [],
                'success': False,
                'error': str(e),
                'start_time': start_time,
                'end_time': end_time,
                'duration': duration
            }

    def process_pages_parallel_batch(self, pages_data: List[Dict[str, Any]], max_workers: int = 3, page_containers: Dict = None, progress_callback=None) -> Dict[int, Dict[str, Any]]:
        """è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ä¸¦åˆ—å‡¦ç†ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰"""
        # å…¨ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
        pages_to_process = pages_data

        if not pages_to_process:
            return {}

        results = {}
        completed_count = 0

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹
            future_to_page = {
                executor.submit(self.process_page_parallel, page_info): page_info['page_num']
                for page_info in pages_to_process
            }

            # çµæœã‚’åé›†ï¼ˆå®Œäº†é †ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œï¼‰
            for future in concurrent.futures.as_completed(future_to_page):
                result = future.result()
                page_num = result['page_num']
                results[page_num] = result
                completed_count += 1
                
                # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ç”¨ï¼‰
                if progress_callback:
                    progress_callback(page_num, result, completed_count, len(pages_to_process))

        return results

    def extract_all_text_fast(self, pdf_path: str) -> List[Dict[str, Any]]:
        """å…¨ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é«˜é€ŸæŠ½å‡ºï¼ˆtext_extractor.pyæ–¹å¼ï¼‰"""
        text_content = []

        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    text_content.append({
                        'page': page_num,
                        'text': text.strip() if text else ''
                    })
        except Exception:
            pass

        return text_content

    def load_screenshot(self, screenshot_path: Path) -> BytesIO:
        """ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
        try:
            with open(screenshot_path, 'rb') as f:
                img_buffer = BytesIO(f.read())
                img_buffer.seek(0)
                return img_buffer
        except Exception:
            return BytesIO()

    def load_screenshots_parallel(self, screenshot_files: List[Path], max_workers: int = 4) -> List[BytesIO]:
        """ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¸¦åˆ—èª­ã¿è¾¼ã¿"""
        screenshots = [BytesIO()] * len(screenshot_files)  # é †åºã‚’ä¿æŒ

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å„ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            future_to_index = {
                executor.submit(self.load_screenshot, screenshot_file): i
                for i, screenshot_file in enumerate(screenshot_files)
            }

            # çµæœã‚’é †åºé€šã‚Šã«æ ¼ç´
            for future in concurrent.futures.as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    img_buffer = future.result()
                    screenshots[index] = img_buffer
                except Exception:
                    screenshots[index] = BytesIO()

        return screenshots

    def get_pdf_hash(self, pdf_path: str) -> str:
        """PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¤å®šç”¨ï¼‰"""
        try:
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
                return hashlib.md5(pdf_bytes).hexdigest()
        except Exception:
            return ""

    def get_cached_texts_or_extract(self, pdf_path: str) -> List[Dict[str, Any]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã‹ã€æ–°è¦æŠ½å‡º"""
        # PDFã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        current_hash = self.get_pdf_hash(pdf_path)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if (current_hash and
            current_hash == st.session_state.get('pdf_hash', '') and
            st.session_state.get('text_cache', {})):

            st.info("ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            return st.session_state.text_cache

        # æ–°è¦æŠ½å‡ºï¼ˆé«˜é€Ÿï¼‰
        st.info("ğŸ“„ PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’é«˜é€ŸæŠ½å‡ºä¸­...")
        text_content = self.extract_all_text_fast(pdf_path)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if current_hash:
            st.session_state.pdf_hash = current_hash
            st.session_state.text_cache = text_content

        return text_content

    def validate_and_clean(self, fixtures: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŠ½å‡ºçµæœã®æ¤œè¨¼ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        cleaned = []

        for fixture in fixtures:
            # è¨˜å·ã®æ¤œè¨¼
            if not fixture.get('è¨˜å·'):
                continue

            # æ•°é‡ã®æ¤œè¨¼
            try:
                fixture['æ•°é‡'] = int(fixture.get('æ•°é‡', 1))
            except:
                fixture['æ•°é‡'] = 1

            # ç©ºæ–‡å­—åˆ—ã®å‡¦ç†
            for key in fixture:
                if fixture[key] == '-' or fixture[key] == 'ï¼':
                    fixture[key] = ''
                elif isinstance(fixture[key], str):
                    fixture[key] = fixture[key].strip()

            cleaned.append(fixture)

        return cleaned

def main():
    st.title("ğŸ—ï¸ å»ºå…·è¡¨æŠ½å‡ºã‚¢ãƒ—ãƒª")
    st.markdown("PDFã‹ã‚‰å»ºå…·è¡¨ã‚’è‡ªå‹•æŠ½å‡ºã—ã¾ã™")

    # Initialize session state
    if 'results' not in st.session_state:
        st.session_state.results = {}

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã®ãŸã‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
    if 'text_cache' not in st.session_state:
        st.session_state.text_cache = {}
    if 'pdf_hash' not in st.session_state:
        st.session_state.pdf_hash = ""

    # Get specific PDF file
    pdf_file = Path("data/image.pdf")

    # Alternative: Use a fixed version if available
    # pdf_file = Path("data/image_fixed.pdf") if Path("data/image_fixed.pdf").exists() else Path("data/image.pdf")

    if not pdf_file.exists():
        st.error("data/image.pdf ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # Main area for PDF processing
    st.info(f"ğŸ“„ å‡¦ç†å¯¾è±¡: {pdf_file.name}")

    # Processing mode selection
    col1, col2 = st.columns(2)
    with col1:
        processing_mode = st.radio(
            "å‡¦ç†ãƒ¢ãƒ¼ãƒ‰",
            ["ğŸš€ ä¸¦åˆ—å‡¦ç†", "ğŸ“ é †æ¬¡å‡¦ç†"],
            index=0,
            help="ä¸¦åˆ—å‡¦ç†: è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’åŒæ™‚ã«å‡¦ç†ï¼ˆé«˜é€Ÿï¼‰\né †æ¬¡å‡¦ç†: 1ãƒšãƒ¼ã‚¸ãšã¤å‡¦ç†ï¼ˆå®‰å®šï¼‰"
        )

    with col2:
        if processing_mode == "ğŸš€ ä¸¦åˆ—å‡¦ç†":
            max_workers = st.slider(
                "åŒæ™‚å‡¦ç†æ•°",
                min_value=1,
                max_value=4,
                value=4,
                help="åŒæ™‚ã«å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ï¼ˆ4ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰"
            )
        else:
            max_workers = 1

    if st.button("ğŸ” æŠ½å‡ºé–‹å§‹", type="primary", use_container_width=True):
        # Initialize parser
        try:
            parser = LLMFixtureParser()

            # Process PDF
            all_fixtures = []
            page_results = {}

            # Use screenshots only
            screenshot_dir = Path("data/pdf_screenshots")
            screenshot_files = sorted(screenshot_dir.glob("page_*.png"))

            if not screenshot_files:
                st.error("ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                st.info("data/pdf_screenshots/page_1.png, page_2.png... ã¨ã—ã¦ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜ã—ã¦ãã ã•ã„")
                return

            st.info("ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™...")
            progress_bar = st.progress(0, text="ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")

            total_pages = len(screenshot_files)

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’é«˜é€ŸæŠ½å‡º
            text_content = parser.get_cached_texts_or_extract(str(pdf_file))

            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¸¦åˆ—èª­ã¿è¾¼ã¿
            progress_bar.progress(0.3, text="ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¸¦åˆ—èª­ã¿è¾¼ã¿ä¸­...")
            screenshot_workers = min(6, max_workers + 2)  # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆèª­ã¿è¾¼ã¿ç”¨
            screenshots = parser.load_screenshots_parallel(screenshot_files, screenshot_workers)

            # ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ¸ˆã¿ï¼‰
            page_data = []
            for i, (text_info, img_buffer) in enumerate(zip(text_content, screenshots)):
                page_num = text_info['page']
                text = text_info['text']

                progress_bar.progress(0.5 + (0.3 * i / len(text_content)), text=f"ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­: {page_num}/{len(text_content)}")

                page_data.append({
                    'page_num': page_num,
                    'image_buffer': img_buffer,
                    'text': text,
                    'has_fixtures': True  # å…¨ãƒšãƒ¼ã‚¸å‡¦ç†å¯¾è±¡
                })

            # ãƒšãƒ¼ã‚¸æ•°ã‚’è¡¨ç¤º
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ç·ãƒšãƒ¼ã‚¸æ•°", f"{len(page_data)} ãƒšãƒ¼ã‚¸")
            with col2:
                st.metric("å‡¦ç†å¯¾è±¡", f"{len(page_data)} ãƒšãƒ¼ã‚¸")

            # Create page containers for status display
            st.markdown("---")
            st.subheader("ğŸ“„ ãƒšãƒ¼ã‚¸åˆ¥å‡¦ç†çŠ¶æ³")

            page_containers = {}
            for page_info in page_data:
                page_num = page_info['page_num']

                # ãƒˆã‚°ãƒ«ã§ãƒšãƒ¼ã‚¸è¡¨ç¤ºã‚’åˆ¶å¾¡
                with st.expander(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num}", expanded=True):
                    # Create columns for page image and results
                    col1, col2 = st.columns([1, 1])

                    with col1:
                        st.markdown(f"**ãƒšãƒ¼ã‚¸ {page_num} ç”»åƒ:**")
                        if 'image_buffer' in page_info and page_info['image_buffer']:
                            # Display PNG image from buffer
                            st.image(page_info['image_buffer'], caption=f"Page {page_num}", use_container_width=True)
                        else:
                            st.info("ãƒšãƒ¼ã‚¸ç”»åƒã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“")

                    with col2:
                        st.markdown("**å‡¦ç†çŠ¶æ³:**")
                        # Create placeholder for results
                        page_containers[page_num] = st.container()
                        with page_containers[page_num]:
                            st.info("ğŸ”„ å‡¦ç†å¾…æ©Ÿä¸­...")

            # Process pages based on selected mode
            if processing_mode == "ğŸš€ ä¸¦åˆ—å‡¦ç†":
                # Parallel processing
                progress_bar.progress(0, text="ä¸¦åˆ—å‡¦ç†ã§å»ºå…·æƒ…å ±ã‚’æŠ½å‡ºä¸­...")

                # Process all pages in parallel
                start_time = time.time()

                # ä¸¦åˆ—å‡¦ç†ã§ãƒãƒƒãƒå®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ¸ˆã¿ã®ãŸã‚é«˜é€Ÿï¼‰
                progress_bar.progress(0.8, text=f"ğŸš€ {max_workers}ã¤ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§Gemini APIä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹...")

                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
                def update_page_result(page_num, result, completed, total):
                    # é€²æ—ãƒãƒ¼æ›´æ–°
                    progress = 0.8 + (0.2 * completed / total)
                    progress_bar.progress(progress, text=f"å‡¦ç†ä¸­: {completed}/{total} ãƒšãƒ¼ã‚¸å®Œäº†")
                    
                    # ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒŠæ›´æ–°
                    if page_containers and page_num in page_containers:
                        with page_containers[page_num]:
                            page_containers[page_num].empty()
                            
                            duration = result.get('duration', 0)

                            if result['success']:
                                fixtures = result['fixtures']
                                if fixtures:
                                    page_results[page_num] = fixtures
                                    all_fixtures.extend(fixtures)
                                    st.success(f"âœ… ãƒšãƒ¼ã‚¸ {page_num}: {len(fixtures)}ä»¶ã®å»ºå…·ã‚’æŠ½å‡º (å‡¦ç†æ™‚é–“: {duration:.1f}ç§’)")
                                    
                                    # æŠ½å‡ºçµæœã‚’å³åº§ã«è¡¨ç¤º
                                    with st.expander(f"ğŸ“‹ æŠ½å‡ºçµæœ", expanded=False):
                                        fixtures_df = pd.DataFrame(fixtures)
                                        st.dataframe(fixtures_df, use_container_width=True)
                                else:
                                    st.info(f"ãƒšãƒ¼ã‚¸ {page_num}: å»ºå…·æƒ…å ±ãªã— (å‡¦ç†æ™‚é–“: {duration:.1f}ç§’)")
                            else:
                                st.error(f"âŒ ãƒšãƒ¼ã‚¸ {page_num}: ã‚¨ãƒ©ãƒ¼ - {result.get('error', '')} (å‡¦ç†æ™‚é–“: {duration:.1f}ç§’)")

                # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œï¼ˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
                parallel_results = parser.process_pages_parallel_batch(
                    page_data, 
                    max_workers=max_workers,
                    page_containers=page_containers,
                    progress_callback=update_page_result
                )

                processing_time = time.time() - start_time

                # Show processing time and statistics
                total_pages = len(page_data)
                processed_pages = len([p for p in page_data if p['has_fixtures']])
                total_fixtures = len(all_fixtures)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å‡¦ç†æ™‚é–“", f"{processing_time:.2f}ç§’")
                with col2:
                    st.metric("å‡¦ç†æ¸ˆã¿ãƒšãƒ¼ã‚¸", f"{processed_pages}/{total_pages}")
                with col3:
                    st.metric("æŠ½å‡ºå»ºå…·æ•°", f"{total_fixtures}ä»¶")

                st.success(f"ğŸš€ ä¸¦åˆ—å‡¦ç†å®Œäº†ï¼")

            else:
                # Sequential processing (original method)
                progress_bar.progress(0, text="å»ºå…·æƒ…å ±ã‚’æŠ½å‡ºä¸­...")

                # Process all pages
                total_to_process = len(page_data)

                start_time = time.time()

                for idx, page_info in enumerate(page_data):
                    page_num = page_info['page_num']

                    # Update progress
                    progress_bar.progress((idx + 1) / total_to_process,
                                        text=f"å‡¦ç†ä¸­: {idx + 1}/{total_to_process} ãƒšãƒ¼ã‚¸")

                    # Extract with Gemini (ç°¡æ½”è¡¨ç¤º)
                    with page_containers[page_num]:
                        page_containers[page_num].empty()
                        try:
                            fixtures = parser.extract_with_gemini(page_info['text'])

                            if fixtures:
                                page_results[page_num] = fixtures
                                all_fixtures.extend(fixtures)
                                st.success(f"âœ… ãƒšãƒ¼ã‚¸ {page_num}: {len(fixtures)}ä»¶ã®å»ºå…·ã‚’æŠ½å‡º")
                            else:
                                st.info(f"ãƒšãƒ¼ã‚¸ {page_num}: å»ºå…·æƒ…å ±ãªã—")

                        except Exception as e:
                            st.error(f"âŒ ãƒšãƒ¼ã‚¸ {page_num}: ã‚¨ãƒ©ãƒ¼")

                processing_time = time.time() - start_time

                # Show processing time and statistics
                total_pages = len(page_data)
                processed_pages = len([p for p in page_data if p['has_fixtures']])
                total_fixtures = len(all_fixtures)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å‡¦ç†æ™‚é–“", f"{processing_time:.2f}ç§’")
                with col2:
                    st.metric("å‡¦ç†æ¸ˆã¿ãƒšãƒ¼ã‚¸", f"{processed_pages}/{total_pages}")
                with col3:
                    st.metric("æŠ½å‡ºå»ºå…·æ•°", f"{total_fixtures}ä»¶")

                st.success(f"ğŸ“ é †æ¬¡å‡¦ç†å®Œäº†ï¼")

            # Clean results
            all_fixtures = parser.validate_and_clean(all_fixtures)

            # Store results
            st.session_state.results[pdf_file.name] = all_fixtures

            st.success("âœ… æŠ½å‡ºå®Œäº†ï¼")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # Display results at the bottom (after processing is complete)
    if st.session_state.results:
        st.markdown("---")
        st.header("ğŸ“Š æŠ½å‡ºçµæœ")

        for pdf_name, fixtures in st.session_state.results.items():
            if fixtures:
                # Convert to DataFrame
                df = pd.DataFrame(fixtures)

                # Display metrics
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("å»ºå…·æ•°", f"{len(fixtures)} ä»¶")
                with col2:
                    st.metric("ç·æ•°é‡", f"{sum(f.get('æ•°é‡', 0) for f in fixtures)} å€‹")

                # Display table
                st.dataframe(df, use_container_width=True)

                # Download button for CSV
                csv = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"{pdf_name.replace('.pdf', '')}_fixtures.csv",
                    mime='text/csv'
                )

                # Calculate accuracy against answer.csv
                st.markdown("---")
                st.subheader("ğŸ¯ æ­£è§£ç‡ã®è©•ä¾¡")

                try:
                    # Load answer.csv
                    answer_df = pd.read_csv("data/answer.csv")

                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å®šç¾©
                    fields = ['æ•°é‡', 'å®¤å', 'å‹å¼', 'è¦‹è¾¼', 'ä»•ä¸Šã’', 'ã‚¬ãƒ©ã‚¹', 'ä»˜å±é‡‘ç‰©', 'å‚™è€ƒ']

                    # è©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ç”¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
                    comparison_matrix = []
                    field_accuracy = {field: {'correct': 0, 'total': 0} for field in fields}

                    total_items = len(answer_df)
                    total_cells = total_items * len(fields)
                    correct_cells = 0

                    for _, answer_row in answer_df.iterrows():
                        symbol = answer_row['è¨˜å·']

                        # Find matching row in extracted data
                        extracted_rows = df[df['è¨˜å·'] == symbol]

                        row_result = {'è¨˜å·': symbol}

                        if not extracted_rows.empty:
                            extracted_row = extracted_rows.iloc[0]

                            # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è©•ä¾¡
                            for field in fields:
                                answer_val = str(answer_row.get(field, '')).strip()
                                extracted_val = str(extracted_row.get(field, '')).strip()

                                # Handle empty values
                                if answer_val in ['nan', 'NaN', '']:
                                    answer_val = ''
                                if extracted_val in ['nan', 'NaN', '']:
                                    extracted_val = ''

                                # è©•ä¾¡
                                field_accuracy[field]['total'] += 1
                                if answer_val == extracted_val:
                                    row_result[field] = 'âœ…'
                                    field_accuracy[field]['correct'] += 1
                                    correct_cells += 1
                                else:
                                    row_result[field] = 'âŒ'
                        else:
                            # æœªæŠ½å‡ºã®å ´åˆã€å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸æ­£è§£
                            for field in fields:
                                row_result[field] = 'âŒ'
                                field_accuracy[field]['total'] += 1

                        comparison_matrix.append(row_result)

                    # å…¨ä½“ã¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥æ­£è§£ç‡ã‚’è¨ˆç®—
                    overall_accuracy = (correct_cells / total_cells) * 100 if total_cells > 0 else 0

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("å…¨ä½“æ­£è§£ç‡", f"{overall_accuracy:.1f}%")
                    with col2:
                        st.metric("æ­£è§£ã‚»ãƒ«æ•°", f"{correct_cells} / {total_cells}")
                    with col3:
                        st.metric("æŠ½å‡ºå»ºå…·æ•°", f"{len(df)} / {total_items} ä»¶")

                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥æ­£è§£ç‡è¡¨ç¤º
                    st.subheader("ğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥æ­£è§£ç‡")
                    field_accuracy_data = []
                    for field in fields:
                        accuracy = (field_accuracy[field]['correct'] / field_accuracy[field]['total']) * 100 if field_accuracy[field]['total'] > 0 else 0
                        field_accuracy_data.append({
                            'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰': field,
                            'æ­£è§£æ•°': field_accuracy[field]['correct'],
                            'ç·æ•°': field_accuracy[field]['total'],
                            'æ­£è§£ç‡(%)': f"{accuracy:.1f}"
                        })

                    field_accuracy_df = pd.DataFrame(field_accuracy_data)
                    st.dataframe(field_accuracy_df, use_container_width=True)

                    # è©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¡¨ç¤º
                    with st.expander("ğŸ“‹ è©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆè¡ŒÃ—åˆ—ï¼‰", expanded=False):
                        st.markdown("**å‡¡ä¾‹:** âœ… = æ­£è§£, âŒ = ä¸æ­£è§£")
                        comparison_df = pd.DataFrame(comparison_matrix)
                        st.dataframe(comparison_df, use_container_width=True)

                        # Download comparison CSV
                        comparison_csv = comparison_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="ğŸ“¥ è©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=comparison_csv,
                            file_name="evaluation_matrix.csv",
                            mime='text/csv'
                        )

                        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥æ­£è§£ç‡ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«
                        field_csv = field_accuracy_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="ğŸ“¥ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥æ­£è§£ç‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=field_csv,
                            file_name="field_accuracy.csv",
                            mime='text/csv'
                        )

                except Exception as e:
                    st.error(f"æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

            else:
                st.info("å»ºå…·è¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    main()
